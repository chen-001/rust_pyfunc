#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¤§å¤‡ä»½æ–‡ä»¶è¯»å–åŠŸèƒ½
æµ‹è¯•å››ç§å¢å¼ºçš„è¯»å–æ–¹æ³•
"""

import sys
import os
sys.path.insert(0, '/home/chenzongwei/rust_pyfunc/python')

import rust_pyfunc as rp
import numpy as np
import time

def test_backup_dataset_info():
    """æµ‹è¯•æ•°æ®é›†ä¿¡æ¯è·å–åŠŸèƒ½"""
    print("=" * 50)
    print("æµ‹è¯•1: è·å–å¤‡ä»½æ–‡ä»¶æ•°æ®é›†ä¿¡æ¯")
    print("=" * 50)
    
    # æ‰¾ä¸€ä¸ªå¤‡ä»½æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    backup_files = []
    for root, dirs, files in os.walk('/home/chenzongwei'):
        for file in files:
            if file.endswith('.backup') or file.endswith('.bak'):
                backup_files.append(os.path.join(root, file))
                if len(backup_files) >= 3:  # åªæµ‹è¯•å‰3ä¸ª
                    break
        if len(backup_files) >= 3:
            break
    
    if not backup_files:
        print("æœªæ‰¾åˆ°ä»»ä½•å¤‡ä»½æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    for backup_file in backup_files:
        try:
            print(f"\næµ‹è¯•æ–‡ä»¶: {backup_file}")
            
            # 1. ä½¿ç”¨æ–°çš„get_backup_dataset_infoå‡½æ•°
            rows, cols, memory_gb, min_date, max_date = rp.get_backup_dataset_info(backup_file, "binary")
            print(f"æ•°æ®é›†ä¿¡æ¯:")
            print(f"  è¡Œæ•°: {rows:,}")
            print(f"  åˆ—æ•°: {cols}")
            print(f"  é¢„è®¡å†…å­˜: {memory_gb:.2f} GB")
            print(f"  æ—¥æœŸèŒƒå›´: {min_date} åˆ° {max_date}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤§æ–‡ä»¶
            if memory_gb > 2.0:
                print(f"âš ï¸  è¿™æ˜¯ä¸€ä¸ªå¤§æ–‡ä»¶ ({memory_gb:.1f}GB)ï¼Œå»ºè®®ä½¿ç”¨åˆ†å—è¯»å–")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
def test_chunked_reading():
    """æµ‹è¯•åˆ†å—è¯»å–åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•2: åˆ†å—è¯»å–åŠŸèƒ½")
    print("=" * 50)
    
    # æ‰¾ä¸€ä¸ªç›¸å¯¹è¾ƒå¤§çš„å¤‡ä»½æ–‡ä»¶
    backup_files = []
    for root, dirs, files in os.walk('/home/chenzongwei'):
        for file in files:
            if file.endswith('.backup') or file.endswith('.bak'):
                file_path = os.path.join(root, file)
                if os.path.getsize(file_path) > 1000000:  # å¤§äº1MBçš„æ–‡ä»¶
                    backup_files.append(file_path)
                    break
        if backup_files:
            break
    
    if not backup_files:
        print("æœªæ‰¾åˆ°å¤§å‹å¤‡ä»½æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    backup_file = backup_files[0]
    print(f"æµ‹è¯•æ–‡ä»¶: {backup_file}")
    
    try:
        # å…ˆè·å–æ•°æ®é›†ä¿¡æ¯
        rows, cols, memory_gb, min_date, max_date = rp.get_backup_dataset_info(backup_file, "binary")
        print(f"æ€»è¡Œæ•°: {rows:,}")
        
        if rows == 0:
            print("æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡åˆ†å—æµ‹è¯•")
            return
        
        # 2. æµ‹è¯•è¡ŒèŒƒå›´è¯»å–
        print("\n2.1 æµ‹è¯•è¡ŒèŒƒå›´è¯»å– (å‰100è¡Œ)")
        start_time = time.time()
        result = rp.query_backup(backup_file, row_range=(0, 100), storage_format="binary")
        end_time = time.time()
        print(f"è¯»å–ç»“æœ: {result.shape if result.size > 0 else 'ç©ºæ•°ç»„'}")
        print(f"è€—æ—¶: {end_time - start_time:.3f}ç§’")
        
        # 3. æµ‹è¯•æœ€å¤§è¡Œæ•°é™åˆ¶
        print("\n2.2 æµ‹è¯•æœ€å¤§è¡Œæ•°é™åˆ¶ (æœ€å¤š50è¡Œ)")
        start_time = time.time()
        result = rp.query_backup(backup_file, max_rows=50, storage_format="binary")
        end_time = time.time()
        print(f"è¯»å–ç»“æœ: {result.shape if result.size > 0 else 'ç©ºæ•°ç»„'}")
        print(f"è€—æ—¶: {end_time - start_time:.3f}ç§’")
        
        # 4. æµ‹è¯•å†…å­˜é™åˆ¶
        print("\n2.3 æµ‹è¯•å†…å­˜é™åˆ¶ (é™åˆ¶0.1GB)")
        start_time = time.time()
        try:
            result = rp.query_backup(backup_file, memory_limit_gb=0.1, storage_format="binary")
            print(f"è¯»å–ç»“æœ: {result.shape if result.size > 0 else 'ç©ºæ•°ç»„'}")
        except Exception as e:
            print(f"ç¬¦åˆé¢„æœŸçš„å†…å­˜é™åˆ¶é”™è¯¯: {e}")
        end_time = time.time()
        print(f"è€—æ—¶: {end_time - start_time:.3f}ç§’")
        
        # 5. æµ‹è¯•è¾ƒå®½æ¾çš„å†…å­˜é™åˆ¶
        print("\n2.4 æµ‹è¯•è¾ƒå®½æ¾çš„å†…å­˜é™åˆ¶ (é™åˆ¶2GB)")
        start_time = time.time()
        result = rp.query_backup(backup_file, memory_limit_gb=2.0, storage_format="binary")
        end_time = time.time()
        print(f"è¯»å–ç»“æœ: {result.shape if result.size > 0 else 'ç©ºæ•°ç»„'}")
        print(f"è€—æ—¶: {end_time - start_time:.3f}ç§’")
        
    except Exception as e:
        print(f"âŒ åˆ†å—è¯»å–æµ‹è¯•å¤±è´¥: {e}")

def test_large_file_scenarios():
    """æµ‹è¯•å¤§æ–‡ä»¶åœºæ™¯"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•3: å¤§æ–‡ä»¶å¤„ç†åœºæ™¯")
    print("=" * 50)
    
    # å¯»æ‰¾è¾ƒå¤§çš„å¤‡ä»½æ–‡ä»¶
    large_files = []
    for root, dirs, files in os.walk('/home/chenzongwei'):
        for file in files:
            if file.endswith('.backup') or file.endswith('.bak'):
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path)
                if file_size > 5000000:  # å¤§äº5MB
                    large_files.append((file_path, file_size))
        if len(large_files) >= 2:
            break
    
    if not large_files:
        print("æœªæ‰¾åˆ°å¤§å‹å¤‡ä»½æ–‡ä»¶ï¼Œåˆ›å»ºæ¨¡æ‹Ÿåœºæ™¯")
        return
    
    # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œæµ‹è¯•æœ€å¤§çš„
    large_files.sort(key=lambda x: x[1], reverse=True)
    
    for file_path, file_size in large_files[:2]:
        print(f"\næµ‹è¯•å¤§æ–‡ä»¶: {file_path}")
        print(f"æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.1f} MB")
        
        try:
            # é¦–å…ˆè·å–æ–‡ä»¶ä¿¡æ¯
            rows, cols, memory_gb, min_date, max_date = rp.get_backup_dataset_info(file_path, "binary")
            print(f"é¢„ä¼°ä¿¡æ¯: {rows:,}è¡Œ, {cols}åˆ—, {memory_gb:.2f}GB")
            
            # å¦‚æœé¢„è®¡å†…å­˜ä½¿ç”¨è¶…è¿‡1GBï¼Œä½¿ç”¨åˆ†å—è¯»å–
            if memory_gb > 1.0:
                print("ğŸ’¡ ä½¿ç”¨åˆ†å—è¯»å–ç­–ç•¥")
                
                # è®¡ç®—åˆé€‚çš„åˆ†å—å¤§å°
                chunk_size = max(1000, int(rows * 0.01))  # 1%çš„æ•°æ®ä½œä¸ºä¸€å—
                print(f"åˆ†å—å¤§å°: {chunk_size:,}è¡Œ")
                
                # è¯»å–ç¬¬ä¸€å—
                result = rp.query_backup(file_path, row_range=(0, chunk_size), storage_format="binary")
                print(f"ç¬¬ä¸€å—è¯»å–æˆåŠŸ: {result.shape if result.size > 0 else 'ç©ºæ•°ç»„'}")
                
                # è¯»å–ä¸­é—´ä¸€å—
                mid_start = rows // 2
                mid_end = min(mid_start + chunk_size, rows)
                result = rp.query_backup(file_path, row_range=(mid_start, mid_end), storage_format="binary")
                print(f"ä¸­é—´å—è¯»å–æˆåŠŸ: {result.shape if result.size > 0 else 'ç©ºæ•°ç»„'}")
                
            else:
                print("âœ… æ–‡ä»¶å¤§å°é€‚ä¸­ï¼Œå¯ä»¥ç›´æ¥è¯»å–")
                result = rp.query_backup(file_path, storage_format="binary")
                print(f"å®Œæ•´è¯»å–æˆåŠŸ: {result.shape if result.size > 0 else 'ç©ºæ•°ç»„'}")
                
        except Exception as e:
            print(f"âŒ å¤§æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¤§å¤‡ä»½æ–‡ä»¶è¯»å–åŠŸèƒ½")
    print("æµ‹è¯•å››ç§æ–°çš„è¯»å–æ–¹æ³•:")
    print("1. get_backup_dataset_info() - è·å–æ•°æ®é›†ä¿¡æ¯")
    print("2. query_backup(row_range=...) - æŒ‰è¡ŒèŒƒå›´è¯»å–")
    print("3. query_backup(max_rows=...) - é™åˆ¶æœ€å¤§è¡Œæ•°")
    print("4. query_backup(memory_limit_gb=...) - è‡ªå®šä¹‰å†…å­˜é™åˆ¶")
    
    try:
        test_backup_dataset_info()
        test_chunked_reading() 
        test_large_file_scenarios()
        
        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
        print("=" * 50)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()